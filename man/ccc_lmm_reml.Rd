% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ccc_repeated.R
\name{ccc_lmm_reml}
\alias{ccc_lmm_reml}
\title{Concordance Correlation via REML (Linear Mixed-Effects Model)}
\usage{
ccc_lmm_reml(
  data,
  ry,
  rind,
  rmet = NULL,
  rtime = NULL,
  interaction = TRUE,
  max_iter = 100,
  tol = 1e-06,
  Dmat = NULL,
  ci = FALSE,
  conf_level = 0.95,
  verbose = FALSE,
  digits = 4,
  use_message = TRUE,
  ar = c("none", "ar1"),
  ar_rho = NA_real_,
  slope = c("none", "subject", "method", "custom"),
  slope_var = NULL,
  slope_Z = NULL,
  drop_zero_cols = TRUE
)
}
\arguments{
\item{data}{A data frame.}

\item{ry}{Character. Response variable name.}

\item{rind}{Character. Subject ID variable name (random intercept).}

\item{rmet}{Character or \code{NULL}. Optional column name of method factor
(added to fixed effects).}

\item{rtime}{Character or \code{NULL}. Optional column name of time factor
(added to fixed effects).}

\item{interaction}{Logical. Include \code{method:time} interaction?
(default \code{TRUE}).}

\item{max_iter}{Integer. Maximum iterations for variance-component updates
(default \code{100}).}

\item{tol}{Numeric. Convergence tolerance on parameter change
(default \code{1e-6}).}

\item{Dmat}{Optional \eqn{n_t \times n_t} numeric matrix to weight/contrast
time levels when computing the fixed-effect dispersion term \eqn{S_B}.
Defaults to the identity.}

\item{ci}{Logical. If \code{TRUE}, return a CI container; limits are computed
by a large-sample delta method for CCC (see \strong{CIs} note below).}

\item{conf_level}{Numeric in \eqn{(0,1)}. Confidence level when
\code{ci = TRUE} (default \code{0.95}).}

\item{verbose}{Logical. If \code{TRUE}, prints a structured summary of the
fitted variance components and \eqn{S_B} for each fit (overall or
pairwise). Default \code{FALSE}.}

\item{digits}{Integer \eqn{(\ge 0)}. Number of decimal places to use in the
printed summary when \code{verbose = TRUE}. Default \code{4}.}

\item{use_message}{Logical. When \code{verbose = TRUE}, choose the printing
mechanism, where \code{TRUE} uses \code{message()} (respects \code{sink()},
easily suppressible via \code{suppressMessages()}), whereas \code{FALSE}
uses \code{cat()} to \code{stdout}. Default \code{TRUE}.}

\item{ar}{Character. Residual correlation structure: \code{"none"} (iid) or
\code{"ar1"} for subject-level AR(1) correlation within contiguous time
runs. Default \code{c("none","ar1")}.}

\item{ar_rho}{Numeric of length 1 in \eqn{(-0.999,\,0.999)} or \code{NA}.
When \code{ar = "ar1"} and \code{ar_rho} is finite, it is treated as fixed.
When \code{ar = "ar1"} and \code{ar_rho = NA}, a one-dimensional profile
optimization is used to estimate \eqn{\rho} (see \strong{AR(1)} in
\strong{Details}). Default \code{NA_real_}.}

\item{slope}{Character. Optional extra random-effect design \eqn{Z}.
With \code{"subject"} a single random slope is added (one column in \eqn{Z});
with \code{"method"} one column per method level is added; with
\code{"custom"} you provide \code{slope_Z} directly. Default
\code{c("none","subject","method","custom")}.}

\item{slope_var}{For \code{slope \%in\% c("subject","method")}, a character
string giving the name of a column in \code{data} used as the slope regressor
(e.g., centered time). It is looked up inside \code{data}; do not
pass the vector itself. NAs are treated as zeros in \eqn{Z}.}

\item{slope_Z}{For \code{slope = "custom"}, a numeric matrix with \eqn{n}
rows (same order as \code{data}) providing the full extra random-effect
design \eqn{Z}. \strong{Note:} all columns of \code{slope_Z} share a
single pooled variance component \eqn{\sigma_Z^2}; per-column variances
are not yet supported (under development). Ignored otherwise.}

\item{drop_zero_cols}{Logical. When \code{slope = "method"}, drop all-zero
columns of \eqn{Z} after subsetting (useful in pairwise fits). Default
\code{TRUE}.}
}
\value{
\itemize{
\item If \code{rmet} is \code{NULL} or has a single level, an object of
class \code{c("ccc","ccc_ci")} (when \code{ci=TRUE}) or
\code{c("ccc","matrix")} with a \eqn{1\times 1} matrix containing the
overall CCC estimate.
\item If \code{rmet} has \eqn{L\geq 2} levels, a symmetric \eqn{L\times L}
matrix with pairwise CCC estimates between methods (diagonal set to 1).
When \code{ci=TRUE}, \code{lwr.ci} and \code{upr.ci} matrices are
included.
}
In all cases, attributes \code{"method"}, \code{"description"},
\code{"package"}, and (if \code{ci=TRUE}) \code{"conf.level"} are set.
When \code{ar="ar1"}, an additional attribute \code{"ar_rho"} is attached:
a scalar (overall) or an \eqn{L\times L} matrix (pairwise) with the
\eqn{\rho} values used/estimated.
}
\description{
Compute Lin's Concordance Correlation Coefficient (CCC) from a linear
mixed-effects model fitted by REML. The fixed-effects part can include
\code{method} and/or \code{time} factors (and optionally their interaction),
while a subject-specific random intercept captures between-subject variation.
The implementation avoids any \eqn{n \times n}
inversions by working with small per-subject systems via the Woodbury
identity.
}
\details{
For measurement \eqn{y_{ij}} on subject \eqn{i} under fixed
levels (method, time), we fit
\deqn{ y = X\beta + Zu + \varepsilon,\qquad
       u \sim N(0,\,G),\ \varepsilon \sim N(0,\,R). }
Here \eqn{Z} is the subject-structured random-effects design and \eqn{G} is
block-diagonal at the subject level with the following \emph{per-subject}
parameterization:
\itemize{
\item one random intercept with variance \eqn{\sigma_A^2};
\item optionally, \emph{method} deviations (one column per method level)
with a common variance \eqn{\sigma_{A\times M}^2} and zero
covariances across levels (i.e., multiple of an identity);
\item optionally, \emph{time} deviations (one column per time level)
with a common variance \eqn{\sigma_{A\times T}^2} and zero
covariances across levels;
\item optionally, an \emph{extra} random effect aligned with \eqn{Z}
(random slope): variance \eqn{\sigma_Z^2} times an identity on the
\eqn{Z}-columns (see \strong{Random-slope \eqn{Z}}).
}
The fixed-effects design is \code{~ 1 + rmet + rtime} and, if
\code{interaction=TRUE}, \code{+ rmet:rtime}.

\strong{Residual correlation \eqn{R}.}
With \code{ar="none"}, \eqn{R=\sigma_E^2 I}. With \code{ar="ar1"},
within-subject residuals follow
\deqn{ \varepsilon_i \sim N\!\big(0,\; \sigma_E^2\,C_i(\rho)\big), }
where \eqn{C_i(\rho)} is an AR(1) correlation matrix along the subject's
observations \emph{after ordering by increasing time level}; ties retain input
order, and any \code{NA} time code breaks the series so each contiguous block
of non-\code{NA} times forms a run. Internally we work with the \emph{precision}
of \eqn{C_i}: for a run of length \eqn{L\ge 2}, the tridiagonal inverse has
\deqn{ (C^{-1})_{11}=(C^{-1})_{LL}=\frac{1}{1-\rho^2},\quad
       (C^{-1})_{tt}=\frac{1+\rho^2}{1-\rho^2}\ (2\le t\le L-1),\quad
       (C^{-1})_{t,t+1}=(C^{-1})_{t+1,t}=\frac{-\rho}{1-\rho^2}. }
It contributes \eqn{1/(1-\rho^2)} on the diagonal. The working inverse
is \eqn{R_i^{-1}=\sigma_E^{-2}\,C_i(\rho)^{-1}}.

\strong{Per-subject Woodbury system.} For subject \eqn{i} with \eqn{n_i}
rows, define the per-subject random-effects design \eqn{U_i} (columns:
intercept, method indicators, time indicators; dimension
\eqn{\,r=1+nm+nt\,}). The core never forms
\eqn{V_i = R_i + U_i G U_i^\top} explicitly. Instead,
\deqn{ M_i \;=\; G^{-1} \;+\; U_i^\top R_i^{-1} U_i, }
and accumulates GLS blocks via rank-\eqn{r} corrections using
\eqn{\,V_i^{-1} = R_i^{-1}-R_i^{-1}U_i M_i^{-1}U_i^\top R_i^{-1}\,}:
\deqn{ X^\top V^{-1} X \;=\; \sum_i \Big[
       X_i^\top R_i^{-1}X_i \;-\; (X_i^\top R_i^{-1}U_i)\,
       M_i^{-1}\,(U_i^\top R_i^{-1}X_i) \Big], }
\deqn{ X^\top V^{-1} y \;=\; \sum_i \Big[
       X_i^\top R_i^{-1}y_i \;-\; (X_i^\top R_i^{-1}U_i)\,M_i^{-1}\,
       (U_i^\top R_i^{-1}y_i) \Big]. }
Solves/inversions use symmetric-PD routines with a tiny diagonal "jitter" and
a pseudo-inverse fallback when needed.

\strong{Random-slope \eqn{Z}.}
Besides \eqn{U_i}, the function can include an extra design \eqn{Z_i} and a
corresponding variance \eqn{\sigma_Z^2}:
\itemize{
\item \code{slope="subject"}: \eqn{Z} has one column (the regressor in
\code{slope_var}); \eqn{Z_{i}} is the subject-\eqn{i} block.
\item \code{slope="method"}: \eqn{Z} has one column per method level;
row \eqn{t} uses the slope regressor if its method equals level \eqn{\ell},
otherwise 0; all-zero columns can be dropped via
\code{drop_zero_cols=TRUE} after subsetting.
\item \code{slope="custom"}: \eqn{Z} is provided fully via \code{slope_Z}.
}
Computations simply augment \eqn{U_i} to \eqn{\tilde U_i=[U_i\ Z_i]} and
\eqn{G^{-1}} to \eqn{\tilde G^{-1}} by appending a diagonal block
\eqn{\sigma_Z^{-2} I_{q_Z}}. The EM updates then include
\deqn{ \sigma_Z^{2\,(new)} \;=\; \frac{1}{m\,q_Z}
      \sum_i \sum_{j=1}^{q_Z}\!\Big( b_{i,\text{extra},j}^2 +
      (M_i^{-1})_{\text{extra},jj} \Big)
      \quad (\text{if } q_Z>0). }
\emph{Interpretation:} \eqn{\sigma_Z^2} represents additional within-subject
variability explained by the slope regressor(s). By design it \emph{does not}
enter the CCC formula below, where CCC targets agreement across methods/time, not
variability along a subject- or method-specific slope.

\strong{EM-style variance-component updates.} With current \eqn{\hat\beta},
residuals \eqn{r_i = y_i - X_i\hat\beta} are formed. The BLUPs and
conditional covariances are
\deqn{ b_i \;=\; M_i^{-1}\,(U_i^\top R_i^{-1} r_i), \qquad
      \mathrm{Var}(b_i\mid y) \;=\; M_i^{-1}. }
Expected squares yield closed-form updates:
\deqn{ \sigma_A^{2\,(new)} \;=\; \frac{1}{m}\sum_i \Big( b_{i,0}^2 +
(M_i^{-1})_{00} \Big), }
\deqn{ \sigma_{A\times M}^{2\,(new)} \;=\; \frac{1}{m\,nm}
      \sum_i \sum_{\ell=1}^{nm}\!\Big( b_{i,\ell}^2 +
      (M_i^{-1})_{\ell\ell} \Big)
      \quad (\text{if } nm>0), }
\deqn{ \sigma_{A\times T}^{2\,(new)} \;=\; \frac{1}{m\,nt}
      \sum_i \sum_{t=1}^{nt}\!\Big( b_{i,t}^2 + (M_i^{-1})_{tt} \Big)
      \quad (\text{if } nt>0), }
\deqn{ \sigma_E^{2\,(new)} \;=\; \frac{1}{n} \sum_i
      \Big( r_i^\top C_i(\rho)^{-1} r_i +
      \mathrm{tr}\!\big(M_i^{-1}\,U_i^\top C_i(\rho)^{-1} U_i\big) \Big), }
where \eqn{C_i(\rho)^{-1}} is the AR(1) precision built on the
time-ordered runs described above (and equals \eqn{I} for iid residuals).
Iterate until the \eqn{\ell_1} change across components is \eqn{<}
\code{tol} or \code{max_iter} is reached.

\strong{Fixed-effect dispersion \eqn{\mathbf{S_B}}.} Method dispersion is
computed from \eqn{\hat\beta} and \eqn{\mathrm{Var}(\hat\beta)} with a
contrast matrix \eqn{L} (columns encode pairwise method differences within
each time level) and an optional time-weighting matrix \eqn{\mathrm{D_m}}:
\deqn{ S_B \;=\;
 \frac{\big(L^\top \hat\beta\big)^\top\,\mathrm{D_m}\,
 \big(L^\top \hat\beta\big)
       \;-\; \mathrm{tr}\!\Big(\big(L\,\mathrm{D_m}\,L^\top\big)\,
       \mathrm{Var}(\hat\beta)\Big)}
      {\,nm\,(nm-1)\,\max(nt,1)\,}, }
truncated at 0. The helper \code{\link{build_L_Dm_cpp}} constructs \eqn{L} so it
aligns exactly with the columns of \eqn{X=\mathrm{model.matrix}(\cdot)}.
For exactly two methods (\eqn{nm=2}), a fast path builds \eqn{L} directly
from the design's column names, where, with interaction, the per-time
difference at time \eqn{j} is
\eqn{\beta_{\text{met2}}+\beta_{\text{met2:time}_j}} (baseline time uses
\eqn{\beta_{\text{met2}}}); while without interaction, the same
\eqn{\beta_{\text{met2}}} is used for all times.

\strong{Concordance correlation coefficient.} The CCC used is defined by
\deqn{ \mathrm{CCC} \;=\;
      \frac{\sigma_A^2 + \sigma_{A\times T}^2}
           {\sigma_A^2 + \sigma_{A\times M}^2 +
            \sigma_{A\times T}^2 + S_B + \sigma_E^2}. }
There are special cases when there is no method factor (or a single level),
then \eqn{S_B=0} and \eqn{\sigma_{A\times M}^2=0}; if there is no
time factor (or a single level), then \eqn{\sigma_{A\times T}^2=0}.
The extra random-effect variance \eqn{\sigma_Z^2} (if used) is \emph{not}
included, where CCC targets agreement across methods/time, not variability along
the user-specified slope.

\strong{CIs / SEs (delta method for CCC).}
Let
\deqn{ \theta \;=\; \big(\sigma_A^2,\ \sigma_{A\times M}^2,\
\sigma_{A\times T}^2,\ \sigma_E^2,\ S_B\big)^\top, }
and write the concordance as
\deqn{ \mathrm{CCC}(\theta) \;=\; \frac{N}{D}
      \;=\; \frac{\sigma_A^2 + \sigma_{A\times T}^2}
                   {\sigma_A^2 + \sigma_{A\times M}^2 +
                   \sigma_{A\times T}^2 + S_B + \sigma_E^2}. }

A first-order (large-sample) standard error follows from the delta method:
\deqn{ \mathrm{Var}\{\widehat{\mathrm{CCC}}\}
      \;\approx\; \nabla \mathrm{CCC}(\hat\theta)^\top\,
                  \mathrm{Var}(\hat\theta)\,
                  \nabla \mathrm{CCC}(\hat\theta), }
with gradient components (using \eqn{N} and \eqn{D} as above)
\deqn{ \frac{\partial\,\mathrm{CCC}}{\partial \sigma_A^2}
      \;=\; \frac{D - N}{D^2}
      \;=\; \frac{\sigma_{A\times M}^2 + S_B + \sigma_E^2}{D^2}, }
\deqn{ \frac{\partial\,\mathrm{CCC}}{\partial \sigma_{A\times M}^2}
      \;=\; -\,\frac{N}{D^2}, \qquad
       \frac{\partial\,\mathrm{CCC}}{\partial \sigma_{A\times T}^2}
      \;=\; \frac{D - N}{D^2}, }
\deqn{ \frac{\partial\,\mathrm{CCC}}{\partial \sigma_E^2}
      \;=\; -\,\frac{N}{D^2}, \qquad
       \frac{\partial\,\mathrm{CCC}}{\partial S_B}
      \;=\; -\,\frac{N}{D^2}. }

\emph{Estimating \eqn{\mathrm{Var}(\hat\theta)}.}
The EM updates write each variance component as an average of per-subject
quantities. For subject \eqn{i},
\deqn{ t_{A,i} \;=\; b_{i,0}^2 + (M_i^{-1})_{00},\qquad
       t_{M,i} \;=\; \frac{1}{nm}\sum_{\ell=1}^{nm}
                       \Big(b_{i,\ell}^2 + (M_i^{-1})_{\ell\ell}\Big), }
\deqn{ t_{T,i} \;=\; \frac{1}{nt}\sum_{j=1}^{nt}
                       \Big(b_{i,j}^2 + (M_i^{-1})_{jj}\Big),\qquad
       s_i \;=\; \frac{r_i^\top C_i(\rho)^{-1} r_i +
       \mathrm{tr}\!\big(M_i^{-1}U_i^\top C_i(\rho)^{-1} U_i\big)}{n_i}, }
where \eqn{b_i = M_i^{-1}(U_i^\top R_i^{-1} r_i)} and
\eqn{M_i = G^{-1} + U_i^\top R_i^{-1} U_i}.
With \eqn{m} subjects, we form the empirical covariance of the stacked
subject vectors and scale by \eqn{m} to approximate the covariance of the
means:
\deqn{ \widehat{\mathrm{Cov}}\!\left(
      \begin{bmatrix} t_{A,\cdot} \\ t_{M,\cdot} \\ t_{T,\cdot} \end{bmatrix}
      \right)
      \;\approx\; \frac{1}{m}\,
       \mathrm{Cov}_i\!\left(
      \begin{bmatrix} t_{A,i} \\ t_{M,i} \\ t_{T,i} \end{bmatrix}\right). }
(Drop rows/columns as needed when \code{nm==0} or \code{nt==0}.)

The residual variance estimator is a weighted mean
\eqn{\hat\sigma_E^2=\sum_i w_i s_i} with \eqn{w_i=n_i/n}. Its variance is
approximated by the variance of a weighted mean of independent terms,
\deqn{ \widehat{\mathrm{Var}}(\hat\sigma_E^2)
      \;\approx\; \Big(\sum_i w_i^2\Big)\,\widehat{\mathrm{Var}}(s_i), }
where \eqn{\widehat{\mathrm{Var}}(s_i)} is the sample variance across
subjects. The method-dispersion term uses the quadratic-form delta already
computed for \eqn{S_B}:
\deqn{ \widehat{\mathrm{Var}}(S_B)
      \;=\; \frac{2\,\mathrm{tr}\!\big((A_{\!fix}\,\mathrm{Var}(\hat\beta))^2\big)
             \;+\; 4\,\hat\beta^\top A_{\!fix}\,\mathrm{Var}(\hat\beta)\,
             A_{\!fix}\,\hat\beta}
                   {\big[nm\,(nm-1)\,\max(nt,1)\big]^2}, }
with \eqn{A_{\!fix}=L\,\mathrm{D_m}\,L^\top}.

\emph{Putting it together.} Assemble
\eqn{\widehat{\mathrm{Var}}(\hat\theta)} by combining the
\eqn{(\sigma_A^2,\sigma_{A\times M}^2,\sigma_{A\times T}^2)} covariance
block from the subject-level empirical covariance, add the
\eqn{\widehat{\mathrm{Var}}(\hat\sigma_E^2)} and
\eqn{\widehat{\mathrm{Var}}(S_B)} terms on the diagonal,
and ignore cross-covariances across these blocks (a standard large-sample
simplification). Then
\deqn{ \widehat{\mathrm{se}}\{\widehat{\mathrm{CCC}}\}
      \;=\; \sqrt{\,\nabla \mathrm{CCC}(\hat\theta)^\top\,
                    \widehat{\mathrm{Var}}(\hat\theta)\,
                    \nabla \mathrm{CCC}(\hat\theta)\,}. }

A two-sided \eqn{(1-\alpha)} normal CI is
\deqn{ \widehat{\mathrm{CCC}} \;\pm\; z_{1-\alpha/2}\,
      \widehat{\mathrm{se}}\{\widehat{\mathrm{CCC}}\}, }
truncated to \eqn{[0,1]} in the output for convenience. When \eqn{S_B} is
truncated at 0 or samples are very small/imbalanced, the normal CI can be
mildly anti-conservative near the boundary; a logit transform for CCC or a
subject-level (cluster) bootstrap can be used for sensitivity analysis.

\strong{Estimating \eqn{\rho} for AR(1).}
If \code{ar="ar1"} and \code{ar_rho=NA}, a 1-D Brent search is used to
profile \eqn{\rho\in[-0.95,0.95]} by repeatedly fitting the model and
maximizing the REML log-likelihood. If the
core does not return a log-likelihood, a mild proxy objective is used. For
pairwise method fits, \eqn{\rho} is profiled \emph{per pair}.
}
\section{Notes on stability and performance}{

All per-subject solves are \eqn{\,r\times r} with \eqn{r=1+nm+nt+q_Z}, so cost
scales with the number of subjects and the fixed-effects dimension rather
than the total number of observations. Solvers use symmetric-PD paths with
a small diagonal ridge and pseudo-inverse fallback, which helps for
tiny/unbalanced subsets and near-boundary estimates. Very small samples or
extreme imbalance can still make \eqn{S_B} numerically delicate; negative
estimates are truncated to 0 by construction. For AR(1), observations are
first ordered by time within subject before building the run-wise precision;
\code{NA} time codes break the correlation run.
}

\examples{
#--------------------------------------------------------------------
## Two methods (no time)
#--------------------------------------------------------------------
set.seed(1)
n_subj <- 30
meth   <- factor(rep(c("A","B"), each = n_subj))
id     <- factor(rep(seq_len(n_subj), times = 2))
sigA <- 1.0; sigE <- 0.5
u  <- rnorm(n_subj, 0, sqrt(sigA))
y  <- c(u + rnorm(n_subj, 0, sqrt(sigE)),
         u + 0.2 + rnorm(n_subj, 0, sqrt(sigE)))
dat <- data.frame(y, id, method = meth)
ccc_rm1 <- ccc_lmm_reml(dat, ry = "y", rind = "id", rmet = "method")
print(ccc_rm1)
summary(ccc_rm1)

# 95\% CI container
ccc_rm2 <- ccc_lmm_reml(dat, ry = "y", rind = "id", rmet = "method", ci = TRUE)
ccc_rm2

#--------------------------------------------------------------------
## Two methods x time (balanced 2x2), with and without interaction
#--------------------------------------------------------------------
dat$time <- factor(rep(rep(c("t1","t2"), each = n_subj/2), times = 2))
ccc_lmm_reml(dat, "y", "id", rmet = "method", rtime = "time",
             interaction = FALSE)
ccc_lmm_reml(dat, "y", "id", rmet = "method", rtime = "time",
             interaction = TRUE, verbose = TRUE)

#--------------------------------------------------------------------
## Random slope by subject: create a centered numeric time within subject
#--------------------------------------------------------------------
dat$t_num <- as.integer(dat$time)
dat$t_c   <- ave(dat$t_num, dat$id, FUN = function(v) v - mean(v))
ccc_lmm_reml(dat, "y", "id", rmet = "method", rtime = "time",
             slope = "subject", slope_var = "t_c",
             ar = "ar1", ar_rho = NA_real_, verbose = TRUE)

#--------------------------------------------------------------------
## Three methods - pairwise CCCs
#--------------------------------------------------------------------
set.seed(2)
n_subj <- 40
id2     <- factor(rep(seq_len(n_subj), times = 3))
method2 <- factor(rep(c("A","B","C"), each = n_subj))
sigA <- 1.2; sigE <- 0.6
u  <- rnorm(n_subj, 0, sqrt(sigA))
mu <- c(A = 0.00, B = 0.15, C = -0.10)
e  <- rnorm(3 * n_subj, 0, sqrt(sigE))
y2 <- u[as.integer(id2)] + unname(mu[method2]) + e
dat3 <- data.frame(y = y2, id = id2, method = method2)
ccc_lmm_reml(dat3, "y", "id", rmet = "method", verbose = TRUE)

# ------------------------------------------------------------------
# AR(1) residual correlation (fixed rho, threads forced to 1)
# When needed: repeated measures over time with serially correlated
# residuals within subject (e.g., values drift smoothly across visits).
# ------------------------------------------------------------------
\donttest{
  # Make threaded math libraries behave during examples/CI
  Sys.setenv(OMP_NUM_THREADS = "1",
             OPENBLAS_NUM_THREADS = "1",
             MKL_NUM_THREADS = "1",
             VECLIB_MAXIMUM_THREADS = "1")
  if (requireNamespace("RhpcBLASctl", quietly = TRUE)) {
    RhpcBLASctl::blas_set_num_threads(1)
    RhpcBLASctl::omp_set_num_threads(1)
  }

  set.seed(10)
  n_subj <- 40
  n_time <- 6                      # ≥ 3 time points recommended for AR(1)
  id  <- factor(rep(seq_len(n_subj), each = n_time))
  tim <- factor(rep(seq_len(n_time),  times = n_subj))
  beta0 <- 0; beta_t <- 0.2
  rho_true <- 0.6; sigE <- 0.7
  y <- numeric(length(id))
  for (i in seq_len(n_subj)) {
    idx <- which(id == levels(id)[i])
    e <- stats::arima.sim(list(ar = rho_true), n = n_time, sd = sigE)
    # small linear trend so AR(1) isn't swallowed by fixed effects
    y[idx] <- beta0 + beta_t*(seq_len(n_time) - mean(seq_len(n_time))) + e
  }
  dat_ar <- data.frame(y = y, id = id, time = tim)
  # Fit with AR(1) and rho fixed (nonzero). Estimation of rho is not
  # implemented yet; use a plausible value (e.g., 0.4–0.8) for sensitivity.
  ccc_lmm_reml(dat_ar, ry = "y", rind = "id", rtime = "time",
               ar = "ar1", ar_rho = 0.6, verbose = TRUE)
}

# ------------------------------------------------------------------
# Random slope by SUBJECT
# When needed: each subject shows a systematic linear change over time
# (e.g., individual-specific trends), regardless of method.
# ------------------------------------------------------------------
set.seed(2)
n_subj <- 60; n_time <- 4
id  <- factor(rep(seq_len(n_subj), each = 2 * n_time))
tim <- factor(rep(rep(seq_len(n_time), times = 2), times = n_subj))
method <- factor(rep(rep(c("A","B"), each = n_time), times = n_subj))
# subject-specific slopes around 0
subj <- as.integer(id)
slope_i <- rnorm(n_subj, 0, 0.15)
slope_vec <- slope_i[subj]
base <- rnorm(n_subj, 0, 1.0)[subj]
tnum <- as.integer(tim)
y <- base + 0.3*(method=="B") + slope_vec*(tnum - mean(seq_len(n_time))) +
     rnorm(length(id), 0, 0.5)
dat_s <- data.frame(y, id, method, time = tim)
# center time within subject (recommended for random slopes)
dat_s$t_num <- as.integer(dat_s$time)
dat_s$t_c   <- ave(dat_s$t_num, dat_s$id, FUN = function(v) v - mean(v))
ccc_lmm_reml(dat_s, "y", "id", rmet = "method", rtime = "time",
             slope = "subject", slope_var = "t_c", verbose = TRUE)

# ------------------------------------------------------------------
# Random slope by METHOD
# When needed: methods drift differently across time (e.g., biases that
# change with time are method-specific).
# ------------------------------------------------------------------
set.seed(3)
n_subj <- 60; n_time <- 4
id  <- factor(rep(seq_len(n_subj), each = 2 * n_time))
tim <- factor(rep(rep(seq_len(n_time), times = 2), times = n_subj))
method <- factor(rep(rep(c("A","B"), each = n_time), times = n_subj))
# method-specific slopes: A ~ 0, B ~ positive drift
slope_m <- ifelse(method=="B", 0.25, 0.00)
base <- rnorm(n_subj, 0, 1.0)[as.integer(id)]
tnum <- as.integer(tim)
y <- base + 0.3*(method=="B") + slope_m*(tnum - mean(seq_len(n_time))) +
     rnorm(length(id), 0, 0.5)
dat_m <- data.frame(y, id, method, time = tim)
dat_m$t_num <- as.integer(dat_m$time)
dat_m$t_c   <- ave(dat_m$t_num, dat_m$id, FUN = function(v) v - mean(v))
ccc_lmm_reml(dat_m, "y", "id", rmet = "method", rtime = "time",
             slope = "method", slope_var = "t_c", verbose = TRUE)

# ------------------------------------------------------------------
# Random slopes for SUBJECT *and* METHOD (custom Z)
# When needed: subjects have their own time trends, AND methods carry
# additional method-specific trends. Supply a custom Z with both parts.
# ------------------------------------------------------------------
set.seed(4)
n_subj <- 50; n_time <- 4
id  <- factor(rep(seq_len(n_subj), each = 2 * n_time))
tim <- factor(rep(rep(seq_len(n_time), times = 2), times = n_subj))
method <- factor(rep(rep(c("A","B"), each = n_time), times = n_subj))
subj <- as.integer(id)
# subject slopes + extra slope on method B
slope_subj <- rnorm(n_subj, 0, 0.12)[subj]
slope_B    <- ifelse(method=="B", 0.18, 0.00)
tnum <- as.integer(tim)
base <- rnorm(n_subj, 0, 1.0)[subj]
y <- base + 0.3*(method=="B") +
     (slope_subj + slope_B) * (tnum - mean(seq_len(n_time))) +
     rnorm(length(id), 0, 0.5)
dat_both <- data.frame(y, id, method, time = tim)
# build Z = [subject_slope | method_A_slope | method_B_slope]
dat_both$t_num <- as.integer(dat_both$time)
dat_both$t_c   <- ave(dat_both$t_num, dat_both$id, FUN = function(v) v - mean(v))
MM <- model.matrix(~ 0 + method, data = dat_both)  # one col per method
Z_custom <- cbind(
  subj_slope   = dat_both$t_c,            # subject slope
  MM * dat_both$t_c                       # method-specific slopes
)
ccc_lmm_reml(dat_both, "y", "id", rmet = "method", rtime = "time",
             slope = "custom", slope_Z = Z_custom, verbose = TRUE)

}
\references{
Lin L (1989). A concordance correlation coefficient to evaluate reproducibility.
\emph{Biometrics}, 45: 255-268.

Lin L (2000). A note on the concordance correlation coefficient.
\emph{Biometrics}, 56: 324-325.

Carrasco JL, Jover L (2003). Estimating the concordance correlation coefficient:
a new approach. \emph{Computational Statistics & Data Analysis}, 47(4): 519-539.
}
\seealso{
\code{\link{build_L_Dm_cpp}} and \code{\link{build_L_Dm_Z_cpp}}
for constructing \eqn{L}/\eqn{D_m}/\eqn{Z}; \code{\link{ccc_pairwise_u_stat}}
for a U-statistic alternative; and \pkg{cccrm} for a reference approach via
\pkg{nlme}.
}
\author{
Thiago de Paula Oliveira
}
